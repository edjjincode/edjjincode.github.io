---
layout: single
title: "[HipHop 분류 프로젝트#2]"
categories: 사이드프로젝트
tag: [CNN LSTM]
toc: true
author_profile: false
sidebar:
nav: "docs"
---

## 데이터 셋 설명:

유튜브 채널 딩고 프리스타일에서 각 아티스트의 킬링버스 음악들을 다운 받았다. 아티스트 선정 기준은 각 장르의 색이 짙은 아티스트들을 뽑았다.

딩고 프리스타일은 10~20분 사이에 각 아티스트들이 본인의 곡의 한 부분 씩 짧게 연결해서 하나로 연결시킨 영상이다. 따라서 하나의 영상에 여러 곡들이 들어있다.

각 영상은 대부분 한 아티스트의 노래들을 담고 있으나 경우에 따라서 여러 아티스트들의 곡이 들어 있는 경우도 있다. 아티스트와 상관 없이 곡들을 일일이 들어보면서 직관적으로 붐뱁, 트랩, 싱잉을 분류하여 라벨링하는 작업을 했다.

![HipHop]({{site.url}}/images/2023-10-15-NasaTurbofan/킬링버스.png){: .align-center}

## 데이터 셋 가공:

유튜브에서 딩고 프리스타일 영상을 wav 파일 형태로 받은 후, 각각의 영상마다 mfcc 값을 구하였다. 구한 mfcc 값과 mapping 값, labels 들을 하나의 딕셔너리 형태로 저장한다.

데이터 경로와 json파일 이름, sample_rate, track_duration 값을 설정한 후후 구한 값을 기반으로 samples_per_Track을 구하였다.

```python
DATASET_PATH = "/content/data_new.json"
JSON_PATH = "data_new.json"
SAMPLE_RATE = 22050
TRACK_DURATION = 60 # measured in seconds
SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION
```

num_segments는 3개의 라벨로 라벨링이 되기 때문에 3으로 설정하였다.

```python
um_mfcc=13
n_fft=2048
hop_length=512
num_segments=3
```

각 노래에서 구한 mapping 값, labels 값, mfcc 값을 json 형태로 저장하였다. 여기서 labels 값은 지도학습에 사용되는 것이므로 손수 라벨링을 거친 값이다.

```python
data ={
     "mapping" : [],
     "labels": [],
     "mfcc": []
 }
```

```python
samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)
num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)
```

```python
def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=1):
  """:dataset_path: 데이터 셋의 path
  : json_path: MFCC값을 json파일로 변환 후 저장하는 Path
  : num_mfcc: 뽑아야 하는 결정계수 수
  : n_fft: FFT를 적용할 때 사용하는 간격
  : hop_length: FFT를 적용할 때 window값
  :num_segments: sample track을 몇개의 segment로 분할할 지"
    """

    # mapping, label, mfcc 값 딕셔너리 생성
    data = {
        "mapping": [],
        "labels": [],
        "mfcc": []
    }

    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)
    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)

    # genre에 속한 모든 sub-folder에 루프를 돈다
    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):

        # genre sub-folder에 해당하도록 한다.
        if dirpath is not dataset_path:

            # 장르의 라벨(sub-folder의 제목)을 mapping 딕셔너리 안에 저장한다.
            semantic_label = dirpath.split("/")[-1]
            data["mapping"].append(semantic_label)
            print("\nProcessing: {}".format(semantic_label))

            # genre sub-folder에 있는 모든 오디오 파일을 연다.
            for f in filenames:

		# 오디오 파일을 로딩한다
                file_path = os.path.join(dirpath, f)
                signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)

                # 오디오 segment에 해당하는 모든 segment를 진행 시킨다
                for d in range(num_segments):

                    # calculate start and finish sample for current segment
                    start = samples_per_segment * d
                    finish = start + samples_per_segment

                    # mfcc 값을 추출한다
                    mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)
                    mfcc = mfcc.T

                    # 오직 필요로 하는 mfcc 값만 추출한다.
                    if len(mfcc) == num_mfcc_vectors_per_segment:
                        data["mfcc"].append(mfcc.tolist())
                        data["labels"].append(i-1)
                        print("{}, segment:{}".format(file_path, d+1))

    # Mfcc 값을 json파일에 저장한다
    with open(json_path, "w") as fp:
        json.dump(data, fp, indent=4)
```

```python
if __name__ == "__main__":
    save_mfcc(DATASET_PATH, JSON_PATH, num_segments=1)
```

## 모델 학습

각 곡들에서 구한 mfcc 값을 활용하여 모델을 돌리는 과정을 거쳤다.

## CNN

```python
import json
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import tensorflow as tf

DATA_PATH = "data_new.json"


def load_data(data_path):
  """
  json 파일에서 트레이닝 데이터 셋을 로딩 한다.
  data_path : json 파일의 path
  return X : 입력값(mfcc 값)
  return y : 타깃 값(라벨 값)
    """

    with open(data_path, "r") as fp:
        data = json.load(fp)

    X = np.array(data["mfcc"])
    y = np.array(data["labels"])
    return X, y


def plot_history(history):
  """
  accuracy/loss에 대한 그래프를 그린다.
  param history: 모델의 history
    """

    fig, axs = plt.subplots(2)

    # create accuracy sublpot
    axs[0].plot(history.history["accuracy"], label="train accuracy")
    axs[0].plot(history.history["val_accuracy"], label="test accuracy")
    axs[0].set_ylabel("Accuracy")
    axs[0].legend(loc="lower right")
    axs[0].set_title("Accuracy eval")

    # create error sublpot
    axs[1].plot(history.history["loss"], label="train error")
    axs[1].plot(history.history["val_loss"], label="test error")
    axs[1].set_ylabel("Error")
    axs[1].set_xlabel("Epoch")
    axs[1].legend(loc="upper right")
    axs[1].set_title("Error eval")

    plt.show()


def prepare_datasets(test_size, validation_size):
  """
  데이타를 train, validation, test set으로 나누고 로딩을 한다.
  : param test_size: training test set의 확률값
  : param validation_size : validation set에 대한 확률값
  :return X_train : training set의 입력값
  :return X_validation : validation의 입력값
  :return X_test : test 데이터 셋의 입력값
  : return y_train : 트레이닝 데이터 셋의 타깃 값
  : return y_validaiton : validation 셋의 타깃 값
  : return y_test: 테스트 타깃 값
    """

    # 데이터를 로딩한다
    X, y = load_data(DATA_PATH)

    # test, training, validation으로 나눈다.
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)
    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)

    # 축을 추가해준다.(차원을 높여준다)
    X_train = X_train[..., np.newaxis]
    X_validation = X_validation[..., np.newaxis]
    X_test = X_test[..., np.newaxis]

    return X_train, X_validation, X_test, y_train, y_validation, y_test


def build_model(input_shape):
  """
  CNN 모델을 돌린다.
  : param input_shape (튜플): 입력값의 모형
  : return model: CNN 모델
  """
    # build network topology
    model = tf.keras.Sequential()

    # 1st conv layer
    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))
    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
    model.add(tf.keras.layers.BatchNormalization())

    # 2nd conv layer
    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
    model.add(tf.keras.layers.BatchNormalization())

    # 3rd conv layer
    model.add(tf.keras.layers.Conv2D(128, (2, 2), activation='relu'))
    model.add(tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))
    model.add(tf.keras.layers.BatchNormalization())

    # flatten output and feed it into dense layer
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(64, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.5))

    # output layer
    model.add(tf.keras.layers.Dense(3, activation='softmax'))

    return model


def predict(model, X, y):
  """
  학습된 모델을 활용한다.
  : param model: 학습된 분류 값
  : param X: 입력값
  : param y: 타깃 값
  """
    #  입력값의 차원을 4차원으로 증가해준다
    X = X[np.newaxis, ...] # array shape (1, 130, 13, 1)

    # 예측을 실행한다.
    prediction = model.predict(X)

    # 최대 값의 index를 구한다.
    predicted_index = np.argmax(prediction, axis=1)

    print("Target: {}, Predicted label: {}".format(y, predicted_index))


if __name__ == "__main__":

    # get train, validation, test splits
    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)

    # create network
    input_shape = (X_train.shape[1], X_train.shape[2], 1)
    model = build_model(input_shape)

    # compile model
    optimiser = tf.keras.optimizers.Adam(learning_rate=0.0001)
    model.compile(optimizer=optimiser,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    model.summary()

    # train model
    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=30)

    # plot accuracy/error for training and validation
    plot_history(history)

    # evaluate model on test set
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
    print('\nTest accuracy:', test_acc)

    # pick a sample to predict from the test set
    X_to_predict = X_test[100]
    y_to_predict = y_test[100]

    # predict sample
    predict(model, X_to_predict, y_to_predict)
```

## LSTM

```python

import json
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

DATA_PATH = "data_new.json"


def load_data(data_path):
  """
  json 파일에서 트레이닝 데이터 셋을 로딩한다.
  : param data_path(str): json file의 경로
  : return X (ndarray): 입력값
  : return y (ndarray): 타깃값
    """

    with open(data_path, "r") as fp:
        data = json.load(fp)

    X = np.array(data["mfcc"])
    y = np.array(data["labels"])
    return X, y


def plot_history(history):
  """
  training/validation 데이터 셋의 그래프를 그린다
  :param history: 모델의 과거
  """

    fig, axs = plt.subplots(2)

    # 정확도를 그래프로 그린다
    axs[0].plot(history.history["accuracy"], label="train accuracy")
    axs[0].plot(history.history["val_accuracy"], label="test accuracy")
    axs[0].set_ylabel("Accuracy")
    axs[0].legend(loc="lower right")
    axs[0].set_title("Accuracy eval")

    # 오류 그래프를 그린다.
    axs[1].plot(history.history["loss"], label="train error")
    axs[1].plot(history.history["val_loss"], label="test error")
    axs[1].set_ylabel("Error")
    axs[1].set_xlabel("Epoch")
    axs[1].legend(loc="upper right")
    axs[1].set_title("Error eval")

    plt.show()


def prepare_datasets(test_size, validation_size):

  """데이터를 train, test, validation 셋으로 나눈다.
  :parma test_size(float) : 테스트 사이즈의 비율을 나눈다
  : param validation_size : validation 셋을 비율로 나눈다.
  : return X_train: 트레이닝 셋 입력값
  : return X_validation : validation 입력값
  : return X_test : 테스트 데이터 셋의 입력값
  : return y_train : 타깃 데이터 셋
  : return y_validation : 타깃 validation set
  : return y_test: target  test set
    """
    # 데이터를 로드한다.
    X, y = load_data(DATA_PATH)

    # train, test, validation으로 나눈다.
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)
    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)

    return X_train, X_validation, X_test, y_train, y_validation, y_test


def build_model(input_shape):
  """
  RNN-LSTM 모델을 만든다.
  :param input_shape (tuple): 입력값의 형태
  : return model: RNN-LSTM 모델
    """

    # build network topology
    model = tf.keras.Sequential()

    # 2 LSTM layers
    model.add(tf.keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True))
    model.add(tf.keras.layers.LSTM(64))

    # dense layer
    model.add(tf.keras.layers.Dense(64, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.3))

    # output layer
    model.add(tf.keras.layers.Dense(10, activation='softmax'))

    return model


if __name__ == "__main__":

    # get train, validation, test splits
    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)

    # create network
    input_shape = (X_train.shape[1], X_train.shape[2]) # 130, 13
    model = build_model(input_shape)

    # compile model
    optimiser = tf.keras.optimizers.Adam(learning_rate=0.0001)
    model.compile(optimizer=optimiser,
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    model.summary()

    # train model
    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=50)

    # plot accuracy/error for training and validation
    plot_history(history)

    # evaluate model on test set
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
    print('\nTest accuracy:', test_acc)

```
