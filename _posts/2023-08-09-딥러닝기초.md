---
layout: single
title: "[딥러닝기초]/"
categories: 딥러닝기초-요약판
tag: [딥러닝기초 python]
toc: true
author_profile: false
sidebar:
  nav: "docs"
---

# 💡 딥러닝 기초-요약판

본 페이지는 아주대학교 OpenCV-딥러닝 수업을 듣고 요약한 내용을 쓴 것이다.

## 🔥 경사하강법:

선형 회귀 분석 목적이 오차를 줄여주는 기울기 및 상수를 찾는 것과 같이 딥러닝에서도 오차를 줄여주는 방향으로 가기 위해 경사하강법이 사용된다.

![경사하강법]({{site.url}}/images/2023-08-09-딥러닝/경사하강법-1.png){: .align-center}

목표하고자 하는 값에 도달하기 위해서는 a, 가중치, 계수를 최소화 하는 값을 찾아나가야 한다. 그러기 위해서는 오차를 최소화 하는 기울기를 찾아야 한다.

![경사하강법_1]({{site.url}}/images/2023-08-09-딥러닝/경사하강법-2.png){: .align-center}

## Learning Rate(학습률):

학습률은 최적의 기울기를 찾을 때 **이동하는 보폭**을 의미한다.

쉽게 예시를 들자면, 시험 공부할 때 채워야 할 진도의 양이라고 할 수 있다.

#### 1. 넓은 진도를 빠르게 훑어보는 경우:

✔ 빠른 진도

✔ 너무 얕게 본다면 모르고 넘어 간 부분들이 있을 수 있음

#### 2. 처음부터 꼼꼼히 보는 학생:

✔ 처음부터 꼼꼼히 천천히

✔ 비용이 많이 발생함

### 스케줄러:

적절한 학습률을 설정하기 위해 학습의 반복횟수에 비례해서 적절한 학습률을 자동으로 설정해 준다.

## 신경망:

신경망을 이용한 딥러닝은 인간의 뇌에서 뇌세포가 신경을 전달하는 과정을 모방한 기술이다.

#### [뇌세포가 작동하는 방식]

![신경세포]({{site.url}}/images/2023-08-09-딥러닝/신경세포.png){: .align-center}

신경세포는 다음과 같은 방식으로 진행된다.

1. 전기 신호가 들어온다. 이때 특정 신호는 다른 연결보다 강하게 들어오거나 약하게 들어온다.

2. 신경 세포로 들어온 전기 신호의 강약 정도에 따라 해당 신호가 역치 값을 지났으면 1, 역치 값을 지나지 못했으면 0으로 디지털 이진 신호를 보낸다.

#### [딥러닝에서 신호가 작동하는 방법]

![신경세포]({{site.url}}/images/2023-08-09-딥러닝/딥러닝_신경세포.png){: .align-center}

딥러닝은 신경망을 모방한 기술이기에 유사한 방식으로 작동한다.

1. (신경세포): 전기신호가 들어온다

   **(딥러닝): 전 layer에 있는 가중치들을 선형변환한다.(아핀 변환한다)**

2. (신경세포): 역치 값에 따라 신호를 1 또는 0으로 전달한다

**(딥러닝): 활성화 함수(activation function)을 활용하여 아핀변환된 값을 0 또는 1로 변환한다.**

## ⭐ 활성화 함수

모든 인공신경망층은 앞서 받아온 신호를 기준으로 0, 1의 출력을 내주는 함수를 가지고 있다.

💡 가장 대표적인 활성화 함수는 Sigmoid 함수이다. sigmoid 함수는 결과 값으로 0/1이 될 확률을 계산하여 출력해준다.

![시그모이드]({{site.url}}/images/2023-08-09-딥러닝/시그모이드.png){: .align-center}

❗하지만, 시그모이드 함수는 Vanishing Gradient라는 한계점이 존재한다

#### [Vanishing Gradient]

시그모이드 함수를 활성화 함수로 사용 시, 신경망의 깊이가 깊어질수록(신경망들이 연결되는 층들이 많아질수록) 기울기 값이 소실되는 문제가 발생한다.

![시그모이드]({{site.url}}/images/2023-08-09-딥러닝/기울기_손실.png){: .align-center}

시그모이드 함수 같은 경우 미분값의 최대 값은 0.3이다. 따라서 layer가 깊어질 수록 layer n개만큼 0.3을 곱하게 되므로 0에 수렴하게 된다는 것을 알 수 있다.

이를 우리의 기억력으로 생각을 하면 이해하기 쉽다. 어제 있었던 일을 기억하는 것은 비교적 쉬우나 1년 전에 있었던 내용을 기억하는 것은 쉽지 않다. 이와 같이 vanishing gradient도 layer가 깊어질수록 그 기울기 값을 잃는다고 생각하면 된다.

이를 해결하기 위해 고안된 활성화 함수들은 다음과 같다. 필요한 상황에 맞게 활성화 함수를 선택하는 게 좋으나, 통상적으로 relu가 가장 많이 활용된다.

![활성화함수]({{site.url}}/images/2023-08-09-딥러닝/활성화함수.png){: .align-center}

#### [활성화 함수 선택의 기준]

통상적으로 Relu 함수가 활성화 함수로 가장 많이 사용되나, 상황에 맞는 활성화 함수를 사용하는 것이 중요하다.

#### <목적에 따른 활성화 함수>

딥러닝은 네트워크 종류에 따라 다른 활성화 함수를 사용하게 된다. 단순 MLP나 CNN을 사용할 시, ReLU 함수를 사용한다. 하지만 RNN을 사용할 때에는 Sigmoid 함수나 Tanh 함수를 사용한다.

![활성화선택기준]({{site.url}}/images/2023-08-09-딥러닝/활성화선택기준.png){: .align-center}

#### <문제 성격에 따라 다른 활성화 함수>

문제의 성격에 따라서도 다른 활성화 함수를 사용해야 한다. 분류 문제는 결과 값이 이진 혹은 다중이기 때문에 그에 맞는 활성화 함수를 사용해야 한다. 회귀 문제 같은 경우, 회귀이기 때문에 Linear activation을 사용해야 한다.

![활성화선택기준1]({{site.url}}/images/2023-08-09-딥러닝/활성화선택기준1.png){: .align-center}

## ⭐ 고급 경사하강법(optimizer):

은닉층을 쌓을 수록(layer 층이 깊어질수록) 생긴 문제점 중에 하나가 vanishing gradient problem이었다. 이를 activation function을 통해 해결하니, 또 생긴 문제점이 바로 계산양이 너무 많아 너무 큰 리소스(시간, 하드웨어) 등이 소요된다는 점이다.

너무 많은 리소스가 소요될 시, 리소스 부족/속도 문제가 발생하게 된다.

해당 문제점을 해결하고자 고안된 방법이 고급 경사하강법이다.

#### [고급 경사하강법]

✔ 기존의 경사하강법은 조정을 한번 할 때마다 데이터 전체를 미분해야 하므로 불필요한 계산도 많아지고 속도가 느려지는 문제가 존재하여 이를 보강하기 위해 고안됨

✔ 옵티마이저(최적 함수)라고도 함

더욱 더 최적화 된 고급 경사하강법에는 다양한 종류가 있다. 가장 대표적인 것은 확률 경사하강법이다. 경사하강법과 확률 경사하강법을 제외하고 경사하강법 종류를 3가지로 크게 나누면 1. 방향을 잘 조절 해주는 고급 경사하강법 2. 학습률의 보폭을 잘 조절해주는 고급 경사하강법 3. 방향, 보폭 둘 다 잘 조절해주는 고급 경사하강법이 있다.

#### [SGD 확률 경사하강법]

기존의 경사하강법과는 다르게 데이터 전체를 이용하는 것이 아니라 랜덤하게 추출한 일부 데이터를 사용한다.

![확률경사하강법]({{site.url}}/images/2023-08-09-딥러닝/SGD확률경사하강법.png){: .align-center}

#### [다양한 확률 경사하강법]

#### 1. 방향을 잘 조절 해주는 고급 경사하강법:

[모멘텀]

✔ 경사하강법에 약간의 가중치를 부여하는 방식
✔ 경사하강 때마다 오차 감소 여부를 기준으로 오차가 감소되는 방향으로 가속도를 실어주는 방식
✔점차 방향이 잡히면서 최적의 해를 향해 다가가는 데 필요한 시간을 상당히 줄여준다.

##### <일반 SGD>

![모멘텀전SGD]({{site.url}}/images/2023-08-09-딥러닝/일반SGD.png){: .align-center}

##### <모멘텀을 적용한 SGD>

![모멘텀SGD]({{site.url}}/images/2023-08-09-딥러닝/모멘텀_sgd.png){: .align-center}

그림에서 확인할 수 있듯이 모멘텀을 활용한 SGD가 더 빠르게 해를 향해 찾아가는 것을 볼 수 있다.

##### [NAG(Nesterov Accelerated Gradient)옵티마이저]

경사하강의 방향을 정하기 전에 미리 계산해서 그래디언트 값을 줄이는 원리로 작동한다.

✔ 불필요한 이동을 줄이기 때문에 계산값이 더 줄어드는 장점이 있다.

#### 2. Lr을 잘 조절해주는 고급 경사하강법

#### [Adagrad]

✔ '적응성 기울기'라는 뜻
✔ 기울기의 업데이트가 너무 잦은 경우, 스스로 학습률(lr)을 조정하여 이동 보폭을 조절하는 방법

#### [RMSProp]

✔ Adagrad의 개선판
✔ 보폭 민감도를 보완하여 더욱 적절히 보폭 크기를 개선함

#### 3. 방향 lr 모두 잘 조절해주는 고급 경사하강법

#### [ADAM]

![ADAM]({{site.url}}/images/2023-08-09-딥러닝/adam.png){: .align-center}

## Loss Function(오차 함수)

딥러닝에서 결국에 궁극적으로 풀고자 하는 문제는 오차 함수에 해당하는 오차를 줄이는 것이다. 이 오차 함수가 최저가 될 수 있도록 하는 과정에서 다양한 경사하강법이 사용되고 활성화 함수가 사용되는 것이다.

활성화 함수는 풀고자 하는 문제 상황에 따라 적절한 함수를 선택하는 것이 중요하다.

![lossfuncion]({{site.url}}/images/2023-08-09-딥러닝/lossfunction.png){: .align-center}

![lossfuncion1]({{site.url}}/images/2023-08-09-딥러닝/lossfunction1.png){: .align-center}

## 모델 설계 및 훈련:

딥러닝 모델은 크게 아래와 같은 방식으로 설계된다.(코드가 이런 구조를 따른다.)

1️⃣ 모델을 선언한다. (model = Sequential())

2️⃣ 인공 뉴런들로 이루어진 층을 쌓음 (model.add(Dense(노드의 수, input_shape=(8,))))

3️⃣ Dense: 각 층을 설정해 줌

4️⃣ Compile: 설계한 층들을 취합하고 모델의 세부 옵션을 설정 (model.compile(loss = "", optimizer = "adam"))

5️⃣ fit: 모델에 데이터를 학습시킨다. (model.fit())

## 모델의 평가:

모델 평가는 어떻게 보면 가장 중요한 부분이라고도 할 수 있다. 결국에는 딥러닝을 통해 만든 모델이 좋은 결과를 가져야 의미 있는 것이기 때문이다.

모델 평가할 때 크게 두 가지를 확인해야 한다.

#### 1. 기준 모델 뀌어넘기:

힘들게 딥러닝 모델을 만들었는 데 하지만 못한 결과가 나오게 되면 어떨까? 이를 평가하기 위한 가장 쉬운 방법은 상식 수준 모델과 비교하는 것이다.

예를 들어, 이진 분류 문제에서 정확도가 50% 정도보다 적게 나온다면 모델이 잘 못 되었다는 것을 상식선에서 이해할 수 있다.

#### 2. 모델 용량 키우기(과대적합 만들기):

딥러닝을 공부하다 보면 언제까지 학습을 시켜야 하는지, 내가 만든 모델이 제대로 된 건지 헷갈릴 때가 있다.

중요한 것은 딥러닝 모델을 만들었으면 모든 모델은 과대적합을 만들 수 있다는 것이다. 만일 모델을 만들었는데 과대적합이 안된다면 잘못된 모델을 만들었다고 할 수 있다.
