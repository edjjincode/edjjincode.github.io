---
layout: single
title: "[네이버부스트AI준비]/GradientDescent/Regularization"
categories: TIL
tag: [네이버프리코스]
toc: true
author_profile: false
sidebar:
nav: "docs"
---

# Gradient descent

## 경사하강법의 메소드들

1. Stochastic gradient descent:

하나의 sample을 가지고 gradient를 업데이트 하는 방법이다.

2. Mini-batch gradient descent:

subset의 sample을 가지고 gradient를 업데이트 하는 방법이다.

3. Batch gradient descent:

whole sample을 가지고 gradient를 업데이트 하는 방법이다.

## Batch-size Matters

![batchsize]({{site.url}}/images/2023-09-01-naver14/batchsize.png){: .align-center}

배치 사이즈가 클수록 위 그림에서 오르쪽 그래프와 같이 그래프가 날카롭게 된다. 반면에 배치 사이즈가 작을수록 위 그림에서 왼쪽 그래프와 같이 그래프가 flatten 되는 것을 알 수 있다.

그래프가 날카로울 수록 training과 validation의 격차가 크게 되고 generalization performance가 떨어지게 된다. 반면에 그래프가 평탄해지면 비교적 training과 validation의 격차가 줄게 되고 generalization performace가 높아지게 된다. 따라서 배치 사이즈가 클수록 모델의 generalization performance가 떨어지고 배치 사이즈가 작을수록 generalization performance가 높아진다.

## 경사하강법 기법 종류들:

1. Momentum:

'관성을 이용해보자는' 아이디어를 사용하는 기법이다. '이전 batch에서 이 방향으로 움직였으면, 그 다음 batch에서도 이 방향으로 움직여 보자'라는 방식으로 작동하는 경사하강법 기법이다.

![momentum]({{site.url}}/images/2023-09-01-naver14/momentum.png){: .align-center}

위 식에서 알 수 있듯이, 베타 방향의 모멘텀을 없애는 것이 아닌 축적 시키는 것을 볼 수 있다.

2. Nesterov Accelerated Gradient:

momentum과 비슷하게 작동하나 momentum의 단점을 보완한 기법이다.

![NAG]({{site.url}}/images/2023-09-01-naver14/NAG.png){: .align-center}

3. Adagrad:

파라미터가 지금까지 얼마나 변했는 지를 확인하고 파라미터가 많이 변한 파라미터에 대해서는 적게 변화 시키고 많이 안 변한 파라미터에 대해서는 많이 변화 시키는 방식으로 학습하는 방식

![Adagrad]({{site.url}}/images/2023-09-01-naver14/adagrad.png){: .align-center}

위 식에서 Gt가 지금까지 각 파라미터가 얼만큼 변했는 지를 저장하는 것이 G에 해당한다.

4. RMSprop

Geoff Hinton이 경험적으로 좋은 경사하강기법을 찾은 것

![rmsprop]({{site.url}}/images/2023-09-01-naver14/rmsprop.png){: .align-center}

5. Adam

Adam은 momentum과 learning rate 둘다 잡은 경사하강 기법이다.

![adam]({{site.url}}/images/2023-09-01-naver14/adam.png){: .align-center}

# Regularization

학습에 방해함으로써 해당 방법론이 테스트 데이터에도 잘 작동할 수 있도록 하는 방법이다.

1. Early Stopping

![earlystopping]({{site.url}}/images/2023-09-01-naver14/earlystopping.png){: .align-center}

학습을 하다 validation loss가 높아질 때 멈추는 기법이다.

2. Parameter Norm

Penalty를 따로 더해줘서 네트워크의 weight을 더 작게 만들자는 것이다. 이를 통해 함수를 더 부드럽게 만들어 generalization이 쉽게 만들어 주려고 하는 것이다.

![Norm]({{site.url}}/images/2023-09-01-naver14/Norm.png){: .align-center}

3. Data Augmentation:

데이터를 최대한 많이 만들어주어서 모델의 정확도를 높이는 방식이다.

![dataaugmentation]({{site.url}}/images/2023-09-01-naver14/dataaugmentation.png){: .align-center}

4. Noise Robustness:

인풋과 weight에 노이즈를 추가하여 성능을 높이는 방법

5. Label Smoothing:

CutMix 기법을 통해 학습 데이터를 증강 시킬 수 있다. CutMix란 input에 해당하는 값들의 일부분을 서로 겹쳐 새로운 이미지를 만들고 라벨링을 할 때는 라벨을 섞어 학습하는 기법이다. 예를 들어, 강아지와 고양이를 분류해주는 문제가 있다고 해보자. 이때 강아지의 그림 일부분과 고양이의 그림 일부분을 합치고 그 라벨 값을 강아지와 고양이의 비율로 나타낸다.

![labelsmoothing]({{site.url}}/images/2023-09-01-naver14/labelsmoothing.png){: .align-center}

위 그림과 같이 강아지와 고양이 사진을 합치고 그 라벨 값을 확률로 나타낸다.

6. Dropout:

뉴럴네트워크의 값을 0으로 무작위로 만들어 학습하는 방식이다.

![dropout]({{site.url}}/images/2023-09-01-naver14/dropout.png){: .align-center}

7. Batch Normalization:

적용하고자 하는 레이어의 stastics를 정규화 시키는 방법이다.
