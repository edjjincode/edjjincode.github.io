---
layout: single
title: "[컴퓨터비전 OpenCV]"
categories: CV, TIL
tag: [Python, CV, TIL]
toc: true
author_profile: false
sidebar:
  nav: "docs"
---

# OpenCV Object Tracking

## Object Tracking

Object Tracking은 영상에서 감지하고자 하는 물체의 위치를 구할 수 있고 보안, 이상감지, 교통 등 많은 곳에 활용 될 수 있다.

트레킹을 공부하기 앞서 선행적으로 알아야 하는 keyword를 소개하겠다.

Detection(검출): 영상에서 찾고자 하는 대상의 위치와 크기를 알아내는 작업

Recognition(인식): 주어진 영상이 무엇인지 판별하는 작업- Classification/Identification

Tracking(추적): 동영상에서 특정 대상의 위치 변화를 알아내는 작업- Mean Shift, Cam Shift, Trackers in OpenCV

1. SOT & MOT

SOT(Single Object Tracking): 객체를 하나 탐지를 할 때 활용된다.

MOT(Multiple Object Tracking): 객체를 여러 개 탐지할 때 사용된다.

2. 객체 탐지와 오브젝트 트레킹의 차이점:

객체 탐지가 이미지나 비디오에서 물체를 식별한다면, 오브젝트 트레킹은 물체의 위치를 실시간으로 탐지하는 방법이다.

3. 오브젝트 트레킹 시 주의할 점:

배경 클러터(Background Clutter): 대상이 배경에 섞여 구분이 어려울 수 있다.

폐색(Occlusion):대상이 다른 물체에 가려지거나 일부분만 보일 수 있다.

축적 변화(Scale variation): 대상의 크기 변화에 따라 다르게 인식 될 수 있다.

학습 시간 및 트레킹 속도: 대상이 너무 빠른 속도로 움직인다던지, 너무 작을 시에는 학습 시키기 어렵다.

### 오브젝트 트레킹의 단계

[Target Initialization]

대상이 되는 물체의 바운딩 박스를 그리는 것이 첫 번째 단계이다. 이후, 트레팅 모델이 바운딩 박스 안에 있는 대상의 위치들을 구할 수 있어야 한다.

[Appearance Modeling]

물체를 바운딩 박스로 지정해주면, 물체가 다른 조명, 각도 또는 속도로 인해 외관이 변경되어 물체가 자체를 잃지 않도록 알고리즘이 변경 및 왜곡을 감지할 수 있도록 해야 한다.

[Motion Estimation]

물체의 향후 위치를 정확하게 예측 할 수 있어야 한다.

[Target Positioning]

Motion Estimation같은 경우, 물체가 존재할 수 있는 가장 가능성 높은 영역을 근사화하는 과정으로, Motion Estimation 단계를 거친 후에는 Target Position 단계에서 visual model을 활용하여 대상의 위치를 보다 정확하게 결정할 수 있다.

### Mean Shift/CAMSHIFT

#### Mean Shift 개념:

Mean Shift 알고리즘은 분포되어 있는 데이터에서 가장 밀집되어 있는 부분을 찾아내는 알고리즘이다.

트레킹을 하고자 하는 대상의 입력 영상을 히스토그램 백투영을 통해 확률적인 정보를 얻어낸다.

![motion_field]({{site.url}}/images/2023-08-09-opencv_3/meanshift.png){: .align-center}

위 그림에서 살색영역을 추적하려고 한다고 가정을 해보자. 빨간색 윈도우가 기존의 위치이다. 빨간색 window안에 있는 값의 중심점을 구하고, 윈도우 안에 있는 픽셀 값의 최대강도 값을 구한다. 구한 중심점과 최대강도 값이 다를 경우, 윈도우를 옮겨 최대강도 값이 윈도우의 중심점이 되도록 한다. 이런식으로 윈도우를 움직여 움직이지 않을 때까지 실행한다.

```python
# Our Setup, Import Libaries, Create our Imshow Function and Download our Images
import cv2
import numpy as np
from matplotlib import pyplot as plt
from google.colab.patches import cv2_imshow

# Define our imshow function
def imshow(title = "Image", image = None, size = 10):
    w, h = image.shape[0], image.shape[1]
    aspect_ratio = w/h
    plt.figure(figsize=(size * aspect_ratio,size))
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.title(title)
    plt.show()

!wget https://github.com/makelove/OpenCV-Python-Tutorial/raw/master/data/slow.flv
```

```python
cap = cv2.VideoCapture('slow.flv')

# take first frame of the video
ret,frame = cap.read()

# Get the height and width of the frame (required to be an interger)
width = int(cap.get(3))
height = int(cap.get(4))

# Define the codec and create VideoWriter object. The output is stored in '*.avi' file.
out = cv2.VideoWriter('car_tracking_mean_shift.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 30, (width, height))

# setup initial location of window
r,h,c,w = 250,90,400,125  # simply hardcoded the values
track_window = (c,r,w,h)

# set up the ROI for tracking
roi = frame[r:r+h, c:c+w]
hsv_roi =  cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
mask = cv2.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))
roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])
cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)

# Setup the termination criteria, either 10 iteration or move by atleast 1 pt
term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )

while(1):
    ret, frame = cap.read()

    if ret == True:
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)

        # apply meanshift to get the new location
        ret, track_window = cv2.meanShift(dst, track_window, term_crit)

        # Draw it on image
        x,y,w,h = track_window
        img2 = cv2.rectangle(frame, (x,y), (x+w,y+h), (255,255,255),2)
        out.write(img2)
        #imshow('Tracking', img2)

    else:
        break

cap.release()
out.release()
```

```python
!ffmpeg -i /content/car_tracking_mean_shift.avi car_tracking_mean_shift.mp4 -y
```

### CAMSHIFT

CAMSHIFT는 민시프트의 단점을 보완한 추적 방법이다. 캠 시프트는
추적하는 객체의 크기가 변하더라도 검색 윈도우의 크기가 고정되어 있는 평균 이동 알고리즘의 단점을 보완하였다.

CAMSHIFT의 동작 방법:

1. 우선 평균 이동 알고리즘으로 이동 위치 계산을 한다
2. 민시프트가 수렴했다고 판단되면, 검색 윈도우의 크기를 약간씩 키운다
3. 키운 윈도우 안에서 객체의 위치를 찾는다.
4. 특정 공간을 가장 잘 표현하는 타원을 만들어서 타원의 크기만큼 윈도우를 키운다.
5. 객체가 작아졌으면 타원의 크기가 작아지게 된다.
6. 검색 윈도우도 타원의 크기에 맞추어 작아지게 된다.
7. 새로운 크기의 윈도우를 이용하여 다시 평균 이동을 수행한다.

```python
cap = cv2.VideoCapture('slow.flv')

# take first frame of the video
ret,frame = cap.read()

# Get the height and width of the frame (required to be an interger)
width = int(cap.get(3))
height = int(cap.get(4))

# Define the codec and create VideoWriter object. The output is stored in '*.avi' file.
out = cv2.VideoWriter('car_tracking_cam_shift.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 30, (width, height))

# setup initial location of window
r,h,c,w = 250,90,400,125  # simply hardcoded the values
track_window = (c,r,w,h)

# set up the ROI for tracking
roi = frame[r:r+h, c:c+w]
hsv_roi =  cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
mask = cv2.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))
roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])
cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)

# Setup the termination criteria, either 10 iteration or move by atleast 1 pt
term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )

while(1):
    ret ,frame = cap.read()

    if ret == True:
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)

        # apply meanshift to get the new location
        ret, track_window = cv2.CamShift(dst, track_window, term_crit)

        # Draw it on image
        pts = cv2.boxPoints(ret)
        pts = np.int0(pts)
        img2 = cv2.polylines(frame,[pts],True, 255,2)
        out.write(img2)
        #imshow('img2',img2)

    else:
        break

cap.release()
out.release()
```

```python
!ffmpeg -i /content/car_tracking_cam_shift.avi car_tracking_cam_shift.mp4 -y
from IPython.display import HTML
from base64 import b64encode

mp4 = open('car_tracking_cam_shift.mp4','rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

HTML("""
<video controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)
```

## Object Tracking

### Optical Flow

#### Optical Flow란?

Optical Flow는 이전 프레임과 현재 프레임의 차이를 활용하여 구하고자 하는 물체의 움직임을 구별해내는 방법이다.

![optical_flow]({{site.url}}/images/2023-08-09-opencv_3/opticalflow.png){: .align-center}

위 그림과 같이 각 시간이 t에서 t+1로 지났을 때 t 픽셀과 t+1의 픽셀의 명암 변화를 고려하여 물체의 움직임을 추정한다. 이때 계산 과정에서 개별 물체의 움직임을 명시적으로 반영하는 것이 아니다.하지만 물체가 움직일 때 물체의 명암 변화가 발생하므로 이를 활용하여 물체의 움직임을 추정한다.

[motion field]

![motion_field]({{site.url}}/images/2023-08-09-opencv_3/motion_field.png){: .align-center}

위 그림과 같이, 구하고자 하는 물체의 움직임을 벡터 형태로 나타내는 것을 모션 필드이다.

![motion_field]({{site.url}}/images/2023-08-09-opencv_3/motionfield.png){: .align-center}

✔ 왼쪽 그림을 픽셀 형태로 변형한 것이 오른쪽 그림이다. 초록색 삼각형이 파란색의 t+1로 이동함에 따라 오른쪽 픽셀 값도 움직인다는 것을 알 수 있다. 어떤 픽셀이 해당 초록색 삼각형에 대응되는 지 추정하는 것이 핵심이다.

```python
!wget https://github.com/rajeevratan84/ModernComputerVision/raw/main/walking_short_clip.mp4
!wget https://github.com/rajeevratan84/ModernComputerVision/raw/main/walking.avi
```

```python
# Load video stream, short clip
#cap = cv2.VideoCapture('walking_short_clip.mp4')

# Load video stream, long clip
cap = cv2.VideoCapture('walking.avi')

# Get the height and width of the frame (required to be an interger)
width = int(cap.get(3))
height = int(cap.get(4))

# Define the codec and create VideoWriter object. The output is stored in '*.avi' file.
out = cv2.VideoWriter('optical_flow_walking.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 30, (width, height))

# Set parameters for ShiTomasi corner detection
feature_params = dict( maxCorners = 100,
                       qualityLevel = 0.3,
                       minDistance = 7,
                       blockSize = 7 )

# Set parameters for lucas kanade optical flow
lucas_kanade_params = dict( winSize  = (15,15),
                  maxLevel = 2,
                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

# Create some random colors
# Used to create our trails for object movement in the image
color = np.random.randint(0,255,(100,3))

# Take first frame and find corners in it
ret, prev_frame = cap.read()
prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)

# Find inital corner locations
prev_corners = cv2.goodFeaturesToTrack(prev_gray, mask = None, **feature_params)

# Create a mask image for drawing purposes
mask = np.zeros_like(prev_frame)

while(1):
    ret, frame = cap.read()

    if ret == True:
      frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

      # calculate optical flow
      new_corners, status, errors = cv2.calcOpticalFlowPyrLK(prev_gray,
                                                            frame_gray,
                                                            prev_corners,
                                                            None,
                                                            **lucas_kanade_params)

      # Select and store good points
      good_new = new_corners[status==1]
      good_old = prev_corners[status==1]

      # Draw the tracks
      for i,(new,old) in enumerate(zip(good_new, good_old)):
          a, b = new.ravel()
          c, d = old.ravel()
          mask  = cv2.line(mask, ( int(a),int(b) ),( int(c),int(d) ), color[i].tolist(), 2)
          frame = cv2.circle(frame, ( int(a),int(b) ), 5, color[i].tolist(),-1)

      img = cv2.add(frame,mask)

      # Save Video
      out.write(img)
      # Show Optical Flow
      #imshow('Optical Flow - Lucas-Kanade',img)

      # Now update the previous frame and previous points
      prev_gray = frame_gray.copy()
      prev_corners = good_new.reshape(-1,1,2)

    else:
      break

cap.release()
out.release()

```

```python
!ffmpeg -i /content/optical_flow_walking.avi optical_flow_walking.mp4 -y
```

```python
from base64 import b64encode

mp4 = open('optical_flow_walking.mp4','rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML("""
<video controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)
```

#### Dense Optical Flow

#### Color Tracking

특정 색상 range를 추적한다

```python
!wget https://moderncomputervision.s3.eu-west-2.amazonaws.com/bmwm4.mp4

#Object Tracking
import cv2
import numpy as np

# Initalize camera
#cap = cv2.VideoCapture(0)

# define range of color in HSV
lower = np.array([20,50,90])
upper = np.array([40,255,255])

# Create empty points array
points = []

# Get default camera window size

# Load video stream, long clip
cap = cv2.VideoCapture('bmwm4.mp4')

# Get the height and width of the frame (required to be an interger)
width = int(cap.get(3))
height = int(cap.get(4))

# Define the codec and create VideoWriter object. The output is stored in '*.avi' file.
out = cv2.VideoWriter('bmwm4_output.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 30, (width, height))

ret, frame = cap.read()
Height, Width = frame.shape[:2]
frame_count = 0
radius = 0

while True:

    # Capture webcame frame
    ret, frame = cap.read()
    if ret:
      hsv_img = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

      # Threshold the HSV image to get only green colors
      mask = cv2.inRange(hsv_img, lower, upper)
      #mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

      contours, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

      # Create empty centre array to store centroid center of mass
      center =   int(Height/2), int(Width/2)

      if len(contours) > 0:

          # Get the largest contour and its center
          c = max(contours, key=cv2.contourArea)
          (x, y), radius = cv2.minEnclosingCircle(c)
          M = cv2.moments(c)

          # Sometimes small contours of a point will cause a divison by zero error
          try:
              center = (int(M["m10"] / M["m00"]), int(M["m01"] / M["m00"]))

          except:
              center =   int(Height/2), int(Width/2)

          # Allow only countors that have a larger than 25 pixel radius
          if radius > 25:

              # Draw cirlce and leave the last center creating a trail
              cv2.circle(frame, (int(x), int(y)), int(radius),(0, 0, 255), 2)
              cv2.circle(frame, center, 5, (0, 255, 0), -1)

          # Log center points
          points.append(center)

      # If radius large enough, we use 25 pixels
      if radius > 25:

          # loop over the set of tracked points
          for i in range(1, len(points)):
              try:
                  cv2.line(frame, points[i - 1], points[i], (0, 255, 0), 2)
              except:
                  pass

          # Make frame count zero
          frame_count = 0

      out.write(frame)
    else:
      break

# Release camera and close any open windows
cap.release()
out.release()

```

```python
!ffmpeg -i /content/bmwm4_output.avi bmwm4_output.mp4 -y
```

```python
from IPython.display import HTML
from base64 import b64encode

mp4 = open('bmwm4_output.mp4','rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

HTML("""
<video controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)
```

#### TrackerCSRT

```python
import cv2
import numpy as np


#tracker = cv2.legacy.TrackerKCF_create()
# tracker = cv2.legacy.TrackerCSRT_create()
tracker = cv2.TrackerCSRT_create()

video = cv2.VideoCapture('Videos/street.mp4')
ok, frame = video.read()

bbox = cv2.selectROI(frame)
#print(bbox)

ok = tracker.init(frame, bbox)
#print(ok)

while True:
    ok, frame = video.read()
    #print(ok)
    if not ok:
        break
    ok, bbox = tracker.update(frame)
    #print(bbox)
    #print(ok)

    if ok:
        (x, y, w, h) = [int(v) for v in bbox]
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0,255,0), 2, 1)
    else:
        cv2.putText(frame, 'Error', (100, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)

    cv2.imshow('Tracking', frame)
    if cv2.waitKey(1) & 0XFF == 27: # ESC
        break

```

### OCR(Optical Character Recognition)

OCR은 스캔된 종이 문서, PDF 파일 또는 디지털 카메라에 의해 캡처된 이미지와 같은 다양한 유형의 문서에서 데이터 추출을 자동화하는 기술이다.

```python
!sudo apt install tesseract-ocr
!pip install pytesseract
```

```python
import cv2
import pytesseract
import numpy as np
from matplotlib import pyplot as plt

pytesseract.pytesseract.tesseract_cmd = (
    r'/usr/bin/tesseract'
)

def imshow(title = "Image", image = None, size = 10):
    w, h = image.shape[0], image.shape[1]
    aspect_ratio = w/h
    plt.figure(figsize=(size * aspect_ratio,size))
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.title(title)
    plt.show()
```
