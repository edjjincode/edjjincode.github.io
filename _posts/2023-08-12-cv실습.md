---
layout: single
title: "[CVê¸°ì´ˆ-ì‹¤ìŠµ]/"
categories: CV, TIL
tag: [CVê¸°ì´ˆ, Python, TIL]
toc: true
author_profile: false
sidebar:
  nav: "docs"
---

# CVê¸°ì´ˆ-ì‹¤ìŠµ

## íŒ¨ì…˜ MNIST ë°ì´í„° ì…‹(ë¶„ë¥˜ ë¬¸ì œ)

### ë°ì´í„° ì¤€ë¹„:

```python

fashion_mnist = tf.keras.datasets.fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
```

íŒ¨ì…˜ MNIST ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.

```python
x_train, x_test = x_train /255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)

ê²°ê³¼ê°’: x_train.shape: (60000, 28, 28)

```

ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ ì •ê·œí™” í•˜ëŠ” ê³¼ì •ì„ ê±°ì¹œë‹¤. 255ë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œ? ì»´í“¨í„°ì—ì„œ ìƒ‰ì„ í‘œí˜„í•  ë•Œ ì´ë¯¸ì§€ ê°’ì´ 0~255 ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” ë° ì´ë¥¼ 0~1 ì‚¬ì´ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ì„œì´ë‹¤.

```python
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print(x_train.shape)

ê²°ê³¼: (60000, 28, 28, 1)
```

ì•ì„œ ì´ë¯¸ì§€ì˜ shapeì„ í™•ì¸ í–ˆì„ ë•Œ, ìƒ‰ìƒ ì°¨ì›ì´ ì—†ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ëŠ” ì´ë¯¸ì§€ê°€ gray scaleë¡œ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì´ë‹¤.(ë§Œì•½ RGBë©´ (60000, 28, 28, 3)ì´ ë˜ì—ˆì„ ê²ƒì´ë‹¤.) ì´ë¥¼ ìœ„í•´ ì°¨ì› 1ì„ ì¶”ê°€í•œë‹¤.

```python
K = len(set(y_train))
print("number of classes:", K)
```

set í•¨ìˆ˜ë¥¼ ì¨ì„œ ì›ì†Œë“¤ë§Œ êµ¬í•˜ì˜€ë‹¤.

### ëª¨ë¸ ì„¤ê³„:

CNN ëª¨ë¸ì€ ë³´í†µì˜ ê²½ìš° í¬ê²Œ êµ¬ê°€ì§€ ë‹¨ê³„ë¡œ ë‚˜ë‰œë‹¤.

**1 ë‹¨ê³„: íŠ¹ì„± ì¶”ì¶œ**

**2 ë‹¨ê³„: FC Layer**

![CVì„¤ê³„]({{site.url}}/images/2023-08-12-cvì‹¤ìŠµ/cvì„¤ê³„.png){: .align-center}

ğŸ’¡ Covolution(ì»¨ë³¼ë£¨ì…˜)ì´ë€?

ì…ë ¥ ì´ë¯¸ì§€ì˜ íŠ¹ì„± ë³€í™˜ì´ë¼ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤.

CNNì—ì„œ Covolutionì˜ ì˜ë¯¸ë¥¼ Correlation Neural Networkë¡œ í•´ì„í•´ë„ ëœë‹¤. í•„í„°ë§ ì‹œ í•„í„°ì— í¬í•©ë˜ì§€ ì•Šì€(ìƒê´€ë„ê°€ ë‚®ì€) íŒ¨í„´ì„ ê³¨ë¼ë‚´ëŠ” ê²ƒì´ë¼ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤.

ğŸ’¡Conv2Dë€?

ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•´ì£¼ëŠ” í•¨ìˆ˜ê°€ Conv2Dí•¨ìˆ˜ì´ë‹¤.
Conv2DëŠ” "2D convolution ì—°ì‚°"ì„ ëœ»í•œë‹¤.
Conv2D(32, (3, 3))ì´ë¼ê³  í•˜ë©´ í•„í„°ì˜ ê°œìˆ˜ê°€ 32ê°œ ìˆê³  3\*3 í•„í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

```python

i = Input(shape = x_train[0].shape)

# feature Extraction(íŠ¹ì„± ì¶”ì¶œ)

x1 = Conv2D(32, (3,3), strides =2, activation= "relu")(i)
x2 = Conv2D(64, (3,3), strides =2, activation = "relu")(x2)
x3 = Conv2D(128, (3,3), strides = 2, activation = "relu")(x3)

x4 = Flatten()(x3) # FC layerì— ë„£ì„ ìˆ˜ ìˆë„ë¡ í•œ ì¤„ë¡œ ë§Œë“œëŠ” ê³¼ì •ì„ ê±°ì¹œë‹¤.

# FC layer

x = Dense(512, activation = "relu")()
x = Dense(K, activation = "softmax")(x)
# ë§ˆì§€ë§‰ì—ëŠ” ê° ë°ì´í„°ê°€ K ì¤‘ ì–´ë–¤ ê²ƒì— ì†í•˜ëŠ” ì§€ ì•Œì•„ì•¼ í•˜ë¯€ë¡œ


model = Model(i, x)
```

ëª¨ë¸ì„ ë§Œë“¤ì–´ì¤€ë‹¤.

ì´ë•Œ ë¶„ë¥˜ì´ê¸° ë•Œë¬¸ì— softmax í•¨ìˆ˜ë¥¼ ì“´ ê²ƒì— ì£¼ì˜í•  í•„ìš”ê°€ ìˆë‹¤.

```python
model.compile(optimizer = "adam",
              loss = "sparse_categorical_crossentropy",metrics = ["accuracy"])

r = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 15)
```

ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³  í•™ìŠµì‹œí‚¨ë‹¤.

```python
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label = 'loss')
plt.plot(r.history['val_loss'],label = 'val_loss')
plt.legend()
```

ë°˜ë³µ íšŸìˆ˜ ë‹¹ ì†ì‹¤ê°’ì„ ì‹œê°í•´ ë³¸ë‹¤. ë§Œë“  ëª¨ë¸ì´ ì˜ ë§Œë“¤ì–´ì¡ŒëŠ” ì§€ í‰ê°€í•´ë³¸ë‹¤.

![training_val]({{site.url}}/images/2023-08-12-cvì‹¤ìŠµ/training_val.png){: .align-center}

ìœ„ ê·¸ë˜í”„ë¥¼ ë³´ë©´, 5ë²ˆì§¸ iteration ì´í›„ì— ê³¼ëŒ€ ì í•©ì´ ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python

from sklearn.metrics import confusion_matrix
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize = False,
                          title = "Confusion matrix",
                          cmap = plt.cm.Blues):
    if normalize:
      cm = cm.astype('float') /cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
    else:
      print("Confusion matrix, without normalization")
    print(cm)

    plt.imshow(cm, interpolation = "nearest", cmap = cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation = 45)
    plt.yticks(tick_marks, clases)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                horizontalalignment="center",
                color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))
```

![cm]({{site.url}}/images/2023-08-12-cvì‹¤ìŠµ/cm.png){: .align-center}

```python
# ë‹¨ìˆœíˆ 0~9 ë¼ê³  ì…ë ¥ëœ ë¼ë²¨ì€ êµ¬ë¶„í•˜ê¸° ì–´ë ¤ìš°ë‹ˆ ì‹¤ì œ ë¼ë²¨ê°’ì„ ì…ë ¥í•´ ì¤€ë‹¤
labels = '''T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot'''.split("\n")
```

```python
# ì œëŒ€ë¡œ ë§ì¶”ì§€ ëª»í•œ ë°ì´í„° ìƒ˜í”Œì„ ëŒì•„ë³´ë©´ì„œ ëª¨ë¸ì„ í‰ê°€í•´ ë³¸ë‹¤
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i].reshape(28,28), cmap='gray')
plt.title("True label: %s Predicted: %s" % (labels[y_test[i]], labels[p_test[i]]));
```

## CIFAR-10 ë°ì´í„° ë¶„ì„

```python

#ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
cifar10 = tf.keras.datasets.cifar10
(x_train, y_train),(x_test, y_test) = cifar10.load_data()

# ë¶„ì„ì„ ìœ„í•´ í›ˆë ¨ ì…‹ê³¼ í…ŒìŠ¤íŠ¸ ì…‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ ì¤€ë‹¤
x_train, x_test = x_train/255.0, x_test/255.0
y_train, y_test = y_train.flatten(), y_test.flatten()
print("x_train.shape:", x_train.shape)
print("y_train.shape:", y_train.shape)

ê²°ê³¼ê°’:
x_train.shape : (50000, 32, 32, 3)
y_train.shape (50000, )
```

```python
K = len(set(y_train))
print("number of clases:", K)
```

```python
# CNN ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤
# ì…ë ¥ì¸µ ì„¤ì •
i = Input(shape = x_train[0].shape)
# ì´ 32ê°œì˜ íŠ¹ì„±ì´ ìˆìœ¼ë¯€ë¡œ 32ê°œ ì¸í’‹ì„ ë°›ì•„ê°„ë‹¤

#Stage 1: ì»¨ë³¼ë£¨ì…˜ìœ¼ë¡œ íŠ¹ì„± ì¶”ì¶œí•˜ê¸°
x = Conv2D(32, (3,3), strides =2, activation = "relu")(i)
x = Conv2D(64, (3,3), strides = 2, activation = "relu")(x)
x = Conv2D(128, (3,3), strides = 2, activation = "relu")(x)

#ì¶”ì¶œí•œ íŠ¹ì„±ì„ ëˆŒëŸ¬ì¤€ë‹¤
x = Flatten()(x)
x = Dropout(0.5)(x)

#Stage 2: ì¶”ì¶œí•œ íŠ¹ì„±ì„ FC layerì— ë„£ëŠ”ë‹¤
x = Dense(1024, activation = "relu")(x)
x = Dropout(0.2)(x)

#ì¶œë ¥ì¸µ ì„¤ì •(CNNì˜ ì¶œë ¥ì¸µì˜ í™œì„±í•¨ìˆ˜ëŠ” softmaxë¡œ ì„¤ì •í•œë‹¤)
x = Dense(K, activation = "softmax")(x)

model = Model(i, x)

```

```python
model.compile(optimizer = "adam",
              loss = "sparse_categorical_crossentropy",
              metrics = ["accuracy"])
```

```python
r = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 15)
```

```python
model.evaluate(x_test,y_test)
```

ìœ„ ëª¨ë¸ì„ ë” ì¢‹ê²Œ ë§Œë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout,\
GlobalMaxPooling2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.models import Model
```

```python
#CNN ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤
#ì…ë ¥ì¸µ ì„¤ì •
i = Input(shape = x_train[0].shape)

# Stage 1: íŠ¹ì„± ì¶”ì¶œí•˜ê¸°
x = Conv2D(32, (3,3), activation = "relu", padding = "same")(i)
x = Conv2D(32, (3,3), activation = "relu", padding = "same")(x)
x = MaxPooling2D((2,2))(x)

x = Conv2D(64, (3,3), activation = "relu", padding = "same")(x)
x = Conv2D(64, (3,3), activation = "relu", padding = "same")(x)
x = MaxPooling2D((2,2))(x)

x = Conv2D(128, (3,3), activation = "relu", padding = "same")(x)
x = Conv2d(128, (3,3), activation = "relu")(x)
x = MaxPooling2D((2,2))(x)

# ì¶”ì¶œí•˜ë‚˜ íŠ¹ì„±ì„ ëˆŒëŸ¬ì¤€ë‹¤
x = Flatten()(x)
x = Dropout(0.5)(x)

#Stage 2: ì¶”ì¶œí•œ íŠ¹ì„±ì„ ANN ë¶„ì„í•˜ê¸°

x = Dense(1024, activation = "relu")(x)
x = Droput(0.2)(x)

#ì¶œë ¥ì¸µ ì„¤ì •
x = Dense(K, activation = "softmax")(x)

model = Model(i, x)

```

```python
model.compile(optimizer = "adam",
              loss = "sparse_categorical_crossentropy",
              metrics = ["accuracy"])
r = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 15)
```

```python
model.evaluate(x_test, y_test)
```

### Batch Normalization(ë°ì´í„° ì •ê·œí™”)

Dropoutì´ë‚˜ Poolingì„ í™œìš©í•´ì„œë„ ëª¨ë¸ì´ ë” ì¢‹ì•„ì§€ë‚˜ Batch Normalizationì„ í•˜ë©´ ë”ìš± ë”
ëª¨ë¸ì´ ì¢‹ì•„ì§ˆ ìˆ˜ ìˆë‹¤.

ë°°ì¹˜ ì •ê·œí™”ëŠ” íŠ¹ì„± ì¶”ì¶œ ì‹œ í™œì„±í•¨ìˆ˜ì˜ ì¶œë ¥ ê°’ì„ ì •ê·œí™”í•˜ëŠ” ì‘ì—…ì„ ì˜ë¯¸í•œë‹¤. í•™ìŠµì„ ì§„í–‰í•  ë•Œ ë§ˆë‹¤ í™œì„±í•¨ìˆ˜ì˜ ì¶œë ¥ê°’ì„ ì •ê·œí™” í•´ì£¼ê¸° ë•Œë¬¸ì— ê°€ì¤‘ì¹˜ ì´ˆê¹ƒê°’ì— ì˜ì¡´í•˜ì§€ ì•Šê²Œ ëœë‹¤. ë˜í•œ ì€ë‹‰ì¸µì—ì„œ ì •ê·œí™”ë¥¼ í•˜ê²Œ ë¨ìœ¼ë¡œì¨ ì…ë ¥ë¶„í¬ê°€ ì…ë ¥ë¶„í¬ê°€ ì¼ì •í•˜ê²Œ ì¬ë°°ì—´ë˜ê³ , ê·¸ë ‡ê¸° ë•Œë¬¸ì— í•™ìŠµë¥ ì„ í¬ê²Œ ì„¤ì •í•´ë„ ë¬¸ì œê°€ ë°œìƒí•  í™•ë¥ ì´ ì ì–´ì§„ë‹¤.

```python
i = Input(shape = x_train[0].shape)

#Stage 1: ì»¨ë³¼ë£¨ì…˜ìœ¼ë¡œ íŠ¹ì„± ì¶”ì¶œí•˜ê¸°
x = Conv2D(32, (3, 3), activation = "relu", padding = "same")(i)
x = BatchNormalization()(x)
x = Conv2D(32, (3,3), activation = "relu", padding = "same")(x)
x = BatchNormalization()(x)
x = MaxPooling((2,2))(x)

x = Conv2D(128, (3, 3), activation='relu', padding = 'same')(x)
x = BatchNormalization()(x)
x = Conv2D(128, (3, 3), activation='relu')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2,2))(x)

# ì¶”ì¶œí•œ íŠ¹ì„±ì„ ëˆŒëŸ¬ì¤€ë‹¤
x = Flatten()(x)
x = Dropout(0.5)(x)

# Stage 2 : ì¶”ì¶œí•œ íŠ¹ì„±ì„ ANN ë¶„ì„í•˜ê¸°
x = Dense(1024, activation='relu')(x)
x = Dropout(0.2)(x)

x = Dense(K, activation='softmax')(x) #CNN ì¶œë ¥ì¸µì˜ í™œì„±í•¨ìˆ˜ëŠ” softmaxë¡œ ì„¤ì •í•œë‹¤

model = Model(i, x)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=15)

model.evaluate(x_test, y_test)

```

### Data Augmentation

CNNì—ì„œëŠ” Data Augmentationì„ í†µí•´ì„œ ë°ì´í„°ë¥¼ ë”ìš± ë” ë§ì´ ìƒì„±í•˜ê³  ë” ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë‹¤.

```python
batch_size = 32
data_generator = \
tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range = 0.1,
                   height_shift_range = 0.1,
                   horizonatl_flip = True)
train_generator = data_generator.flow(x_train, y_train, batch_size)

steps_per_epoch = x_train.shape[0] // batch_size

r = model.fit(train_generator, validation_data = (x_test, y_test), steps_per_epoch = steps_per_epoch, epochs = 50)
```

ImageDataGenerator: ì‹¤ì‹œê°„ ë°ì´í„° ì¦ê°•ì„ ì‚¬ìš©í•´ì„œ í…ì„œ ì´ë¯¸ì§€ ë°ì´í„° ë°°ì¹˜ë¥¼ ìƒì„±í•œë‹¤. ì•ˆì— ìˆëŠ” íŒŒë¼ë¯¸í„° ê°’ë“¤ì€ ë°ì´í„° ì¦ê°•ì„ í•  ë•Œ ì„¤ì •í•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ì´ë‹¤.

### L1, L2 ê·œì œ

ëª¨ë¸ì˜ ë³µì¡ë„ì— ì œí•œì„ ë‘ì–´ ê°€ì¤‘ì¹˜ì˜ ì‘ì€ ê°’ì„ ê°€ì§€ë„ë¡ ê°•ì œí•  ìˆ˜ ìˆë‹¤. ëª¨ë¸ì˜ ì†ì‹¤ í•¨ìˆ˜ì— í° ê°€ì¤‘ì¹˜ì— ì—°ê´€ëœ ë¹„ìš©ì„ ì¶”ê°€í•œë‹¤. ì´ëŸ¬í•œ ë¹„ìš©ì€ ë‘ê°€ì§€ í˜•íƒœê°€ ìˆë‹¤.

L1 ê·œì œ: ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ì— ë¹„ë¡€í•˜ëŠ” ë¹„ìš©ì´ ì¶”ê°€ëœë‹¤.
L2 ê·œì œ: ê°€ì¤‘ì¹˜ì˜ ì œê³±ì— ë¹„ë¡€í•˜ëŠ” ë¹„ìš©ì´ ì¶”ê°€ëœë‹¤.

L1 ê·œì œì™€ L2 ê·œì œ ëª¨ë‘ í˜ë„í‹° í•­ì€ í›ˆë ¨í•  ë•Œë§Œ ì¶”ê°€ëœë‹¤. ì´ ëª¨ë¸ì˜ ì†ì‹¤ì€ í…ŒìŠ¤íŠ¸ë³´ë‹¤ í›ˆë ¨í•  ë•Œ ë” ë†’ì„ ê²ƒì´ë‹¤.

```python
from tensorflow.keras.datasets import fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

classes = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

```

```python
img_rows = x_train[0].shape[0]
img_cols = x_train[0].shape[1]
input_shape = (img_rows, img_cols, 1)
x_test /= 255.0
```

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras import regularizers

L2 = 0.001

model = Sequential()

model.add(Conv2D(32, kernel_size = (3,3),
                     activation = "relu",
                     kernel_regularizer = regularizers.l2(L2),
                     input_shape = (32, 32)))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), activation = "relu", kernel_regularizer = regularizers.l2(L2)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation = "relu", kernel_regularizer = regularizers.l2(L2)))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation = "softmax"))

model.compile(loss = "category_crossentropy",
              optimizer = tf.keras.optimizers.SGD(0.001, momentum = 0.9),
              metrics = ["accuracy"])

model.summary()
```

**[ModelCheckpoint]**

ModelCheckpointëŠ” keras ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë™ì•ˆ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•˜ê³ , ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì¸ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.

```python

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

checkpoint = ModelCheckpoint("best_model.h5",
                             monitor = "val_loss",
                             mode = "min",
                             save_best_only = True,
                             verbose = 1)

earlystop = EarlyStopping(monitor = "val_accuracy",
                          patience = 5,
                          min_delta = 0,
                          restore_best_weights = True)

reduce_lr = ReduceLROnPlateau(monitor = "val_loss",
                              factor = 0.2,
                              patience = 3,
                              min_delta = 0.001,
                              )

```
