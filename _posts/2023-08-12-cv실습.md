---
layout: single
title: "[CV기초-실습]/"
categories: CV, TIL
tag: [CV기초, Python, TIL]
toc: true
author_profile: false
sidebar:
  nav: "docs"
---

# CV기초-실습

## 패션 MNIST 데이터 셋(분류 문제)

### 데이터 준비:

```python

fashion_mnist = tf.keras.datasets.fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
```

패션 MNIST 데이터를 불러온다.

```python
x_train, x_test = x_train /255.0, x_test / 255.0
print("x_train.shape:", x_train.shape)

결과값: x_train.shape: (60000, 28, 28)

```

불러온 데이터를 정규화 하는 과정을 거친다. 255로 나누는 이유는 무엇일까? 컴퓨터에서 색을 표현할 때 이미지 값이 0~255 값을 나타내는 데 이를 0~1 사이로 바꾸기 위해서이다.

```python
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print(x_train.shape)

결과: (60000, 28, 28, 1)
```

앞서 이미지의 shape을 확인 했을 때, 색상 차원이 없는 것을 알 수 있다. 이는 이미지가 gray scale로 되어 있기 때문이다.(만약 RGB면 (60000, 28, 28, 3)이 되었을 것이다.) 이를 위해 차원 1을 추가한다.

```python
K = len(set(y_train))
print("number of classes:", K)
```

set 함수를 써서 원소들만 구하였다.

### 모델 설계:

CNN 모델은 보통의 경우 크게 구가지 단계로 나뉜다.

**1 단계: 특성 추출**

**2 단계: FC Layer**

![CV설계]({{site.url}}/images/2023-08-12-cv실습/cv설계.png){: .align-center}

💡 Covolution(컨볼루션)이란?

입력 이미지의 특성 변환이라고 해석할 수 있다.

CNN에서 Covolution의 의미를 Correlation Neural Network로 해석해도 된다. 필터링 시 필터에 포합되지 않은(상관도가 낮은) 패턴을 골라내는 것이라고 해석할 수 있다.

💡Conv2D란?

컨볼루션 레이어를 추가해주는 함수가 Conv2D함수이다.
Conv2D는 "2D convolution 연산"을 뜻한다.
Conv2D(32, (3, 3))이라고 하면 필터의 개수가 32개 있고 3\*3 필터를 사용하는 것을 의미한다.

```python

i = Input(shape = x_train[0].shape)

# feature Extraction(특성 추출)

x1 = Conv2D(32, (3,3), strides =2, activation= "relu")(i)
x2 = Conv2D(64, (3,3), strides =2, activation = "relu")(x2)
x3 = Conv2D(128, (3,3), strides = 2, activation = "relu")(x3)

x4 = Flatten()(x3) # FC layer에 넣을 수 있도록 한 줄로 만드는 과정을 거친다.

# FC layer

x = Dense(512, activation = "relu")()
x = Dense(K, activation = "softmax")(x)
# 마지막에는 각 데이터가 K 중 어떤 것에 속하는 지 알아야 하므로


model = Model(i, x)
```

모델을 만들어준다.

이때 분류이기 때문에 softmax 함수를 쓴 것에 주의할 필요가 있다.

```python
model.compile(optimizer = "adam",
              loss = "sparse_categorical_crossentropy",metrics = ["accuracy"])

r = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 15)
```

모델을 컴파일하고 학습시킨다.

```python
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label = 'loss')
plt.plot(r.history['val_loss'],label = 'val_loss')
plt.legend()
```

반복 횟수 당 손실값을 시각해 본다. 만든 모델이 잘 만들어졌는 지 평가해본다.

![training_val]({{site.url}}/images/2023-08-12-cv실습/training_val.png){: .align-center}

위 그래프를 보면, 5번째 iteration 이후에 과대 적합이 되는 것을 확인할 수 있다.

```python

from sklearn.metrics import confusion_matrix
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize = False,
                          title = "Confusion matrix",
                          cmap = plt.cm.Blues):
    if normalize:
      cm = cm.astype('float') /cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
    else:
      print("Confusion matrix, without normalization")
    print(cm)

    plt.imshow(cm, interpolation = "nearest", cmap = cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation = 45)
    plt.yticks(tick_marks, clases)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                horizontalalignment="center",
                color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))
```

![cm]({{site.url}}/images/2023-08-12-cv실습/cm.png){: .align-center}

```python
# 단순히 0~9 라고 입력된 라벨은 구분하기 어려우니 실제 라벨값을 입력해 준다
labels = '''T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot'''.split("\n")
```

```python
# 제대로 맞추지 못한 데이터 샘플을 돌아보면서 모델을 평가해 본다
misclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i].reshape(28,28), cmap='gray')
plt.title("True label: %s Predicted: %s" % (labels[y_test[i]], labels[p_test[i]]));
```

## CIFAR-10 데이터 분석

```python

#데이터 불러오기
cifar10 = tf.keras.datasets.cifar10
(x_train, y_train),(x_test, y_test) = cifar10.load_data()

# 분석을 위해 훈련 셋과 테스트 셋으로 데이터를 나누어 준다
x_train, x_test = x_train/255.0, x_test/255.0
y_train, y_test = y_train.flatten(), y_test.flatten()
print("x_train.shape:", x_train.shape)
print("y_train.shape:", y_train.shape)

결과값:
x_train.shape : (50000, 32, 32, 3)
y_train.shape (50000, )
```

```python
K = len(set(y_train))
print("number of clases:", K)
```

```python
# CNN 모델을 구축한다
# 입력층 설정
i = Input(shape = x_train[0].shape)
# 총 32개의 특성이 있으므로 32개 인풋을 받아간다

#Stage 1: 컨볼루션으로 특성 추출하기
x = Conv2D(32, (3,3), strides =2, activation = "relu")(i)
x = Conv2D(64, (3,3), strides = 2, activation = "relu")(x)
x = Conv2D(128, (3,3), strides = 2, activation = "relu")(x)

#추출한 특성을 눌러준다
x = Flatten()(x)
x = Dropout(0.5)(x)

#Stage 2: 추출한 특성을 FC layer에 넣는다
x = Dense(1024, activation = "relu")(x)
x = Dropout(0.2)(x)

#출력층 설정(CNN의 출력층의 활성함수는 softmax로 설정한다)
x = Dense(K, activation = "softmax")(x)

model = Model(i, x)

```

```python
model.compile(optimizer = "adam",
              loss = "sparse_categorical_crossentropy",
              metrics = ["accuracy"])
```

```python
r = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 15)
```

```python
model.evaluate(x_test,y_test)
```

위 모델을 더 좋게 만들면 다음과 같다.

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout,\
GlobalMaxPooling2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.models import Model
```

```python
#CNN 모델을 구축한다
#입력층 설정
i = Input(shape = x_train[0].shape)

# Stage 1: 특성 추출하기
x = Conv2D(32, (3,3), activation = "relu", padding = "same")(i)
x = Conv2D(32, (3,3), activation = "relu", padding = "same")(x)
x = MaxPooling2D((2,2))(x)

x = Conv2D(64, (3,3), activation = "relu", padding = "same")(x)
x = Conv2D(64, (3,3), activation = "relu", padding = "same")(x)
x = MaxPooling2D((2,2))(x)

x = Conv2D(128, (3,3), activation = "relu", padding = "same")(x)
x = Conv2d(128, (3,3), activation = "relu")(x)
x = MaxPooling2D((2,2))(x)

# 추출하나 특성을 눌러준다
x = Flatten()(x)
x = Dropout(0.5)(x)

#Stage 2: 추출한 특성을 ANN 분석하기

x = Dense(1024, activation = "relu")(x)
x = Droput(0.2)(x)

#출력층 설정
x = Dense(K, activation = "softmax")(x)

model = Model(i, x)

```

```python
model.compile(optimizer = "adam",
              loss = "sparse_categorical_crossentropy",
              metrics = ["accuracy"])
r = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 15)
```

```python
model.evaluate(x_test, y_test)
```

### Batch Normalization(데이터 정규화)

Dropout이나 Pooling을 활용해서도 모델이 더 좋아지나 Batch Normalization을 하면 더욱 더
모델이 좋아질 수 있다.

배치 정규화는 특성 추출 시 활성함수의 출력 값을 정규화하는 작업을 의미한다. 학습을 진행할 때 마다 활성함수의 출력값을 정규화 해주기 때문에 가중치 초깃값에 의존하지 않게 된다. 또한 은닉층에서 정규화를 하게 됨으로써 입력분포가 입력분포가 일정하게 재배열되고, 그렇기 때문에 학습률을 크게 설정해도 문제가 발생할 확률이 적어진다.

```python
i = Input(shape = x_train[0].shape)

#Stage 1: 컨볼루션으로 특성 추출하기
x = Conv2D(32, (3, 3), activation = "relu", padding = "same")(i)
x = BatchNormalization()(x)
x = Conv2D(32, (3,3), activation = "relu", padding = "same")(x)
x = BatchNormalization()(x)
x = MaxPooling((2,2))(x)

x = Conv2D(128, (3, 3), activation='relu', padding = 'same')(x)
x = BatchNormalization()(x)
x = Conv2D(128, (3, 3), activation='relu')(x)
x = BatchNormalization()(x)
x = MaxPooling2D((2,2))(x)

# 추출한 특성을 눌러준다
x = Flatten()(x)
x = Dropout(0.5)(x)

# Stage 2 : 추출한 특성을 ANN 분석하기
x = Dense(1024, activation='relu')(x)
x = Dropout(0.2)(x)

x = Dense(K, activation='softmax')(x) #CNN 출력층의 활성함수는 softmax로 설정한다

model = Model(i, x)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=15)

model.evaluate(x_test, y_test)

```

### Data Augmentation

CNN에서는 Data Augmentation을 통해서 데이터를 더욱 더 많이 생성하고 더 좋은 모델을 만들어 낼 수 있다.

```python
batch_size = 32
data_generator = \
tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range = 0.1,
                   height_shift_range = 0.1,
                   horizonatl_flip = True)
train_generator = data_generator.flow(x_train, y_train, batch_size)

steps_per_epoch = x_train.shape[0] // batch_size

r = model.fit(train_generator, validation_data = (x_test, y_test), steps_per_epoch = steps_per_epoch, epochs = 50)
```

ImageDataGenerator: 실시간 데이터 증강을 사용해서 텐서 이미지 데이터 배치를 생성한다. 안에 있는 파라미터 값들은 데이터 증강을 할 때 설정할 수 있는 파라미터이다.

### L1, L2 규제

모델의 복잡도에 제한을 두어 가중치의 작은 값을 가지도록 강제할 수 있다. 모델의 손실 함수에 큰 가중치에 연관된 비용을 추가한다. 이러한 비용은 두가지 형태가 있다.

L1 규제: 가중치의 절댓값에 비례하는 비용이 추가된다.
L2 규제: 가중치의 제곱에 비례하는 비용이 추가된다.

L1 규제와 L2 규제 모두 페널티 항은 훈련할 때만 추가된다. 이 모델의 손실은 테스트보다 훈련할 때 더 높을 것이다.

```python
from tensorflow.keras.datasets import fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

classes = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

```

```python
img_rows = x_train[0].shape[0]
img_cols = x_train[0].shape[1]
input_shape = (img_rows, img_cols, 1)
x_test /= 255.0
```

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras import regularizers

L2 = 0.001

model = Sequential()

model.add(Conv2D(32, kernel_size = (3,3),
                     activation = "relu",
                     kernel_regularizer = regularizers.l2(L2),
                     input_shape = (32, 32)))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), activation = "relu", kernel_regularizer = regularizers.l2(L2)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation = "relu", kernel_regularizer = regularizers.l2(L2)))
model.add(Dropout(0.2))
model.add(Dense(num_classes, activation = "softmax"))

model.compile(loss = "category_crossentropy",
              optimizer = tf.keras.optimizers.SGD(0.001, momentum = 0.9),
              metrics = ["accuracy"])

model.summary()
```

**[ModelCheckpoint]**

ModelCheckpoint는 keras 모델을 학습하는 동안 일정한 간격으로 모델의 가중치를 저장하고, 최상의 성능을 보인 모델을 선택하는 기능을 제공한다.

```python

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

checkpoint = ModelCheckpoint("best_model.h5",
                             monitor = "val_loss",
                             mode = "min",
                             save_best_only = True,
                             verbose = 1)

earlystop = EarlyStopping(monitor = "val_accuracy",
                          patience = 5,
                          min_delta = 0,
                          restore_best_weights = True)

reduce_lr = ReduceLROnPlateau(monitor = "val_loss",
                              factor = 0.2,
                              patience = 3,
                              min_delta = 0.001,
                              )

```
