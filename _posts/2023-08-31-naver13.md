layout: single
title: "[네이버부스트AI준비]/확률론"
categories: 네이버부스트AI
tag: [Python]
toc: true
author_profile: false
sidebar:
nav: "docs"

---

## Optimization:

### Generalization

Generalization이 좋다는 의미는 학습 데이터와 차이가 적다는 의미이다. 따라서 학습 데이터가 성능이 안좋다면 Generalization 성능이 좋다한들 안 좋은 모델이라 할 수 있다.

### Bias and Variance

Bias: 평균적으로 입력 값을 주었을 때 출력값과 true target과의 거리

Variance: 입력을 줬을 때 출력이 얼마나 일관적으로 나오는 지를 의미한다.

**[Bias and Variance Tradeoff]**

![트레이드오프]({{site.url}}/images/2023-08-31-naver13/biasvariancetradeoff.png){: .align-center}

Noise가 있는 Cost에서 Cost 값이 줄었을 때, bias와 variance 둘다 주는 것은 어렵다는 이론. bias가 줄었으면 variance가 커지고, bias가 커지면 variance가 준다는 이론이다.

### Bootstrapping:

학습 데이터가 고정 되었을 때 subsampling을 통해 데이터를 여러 개 만들고 그걸 통해 여러 모델을 만들고 metric을 만드는 것을 Bootstrapping이라 한다.

### Bagging(Bootstrapping aggregating) VS Boosting

Bagging:

부트스트래핑을 통해 나온 여러 예측 값들을 voting 혹은 평균 값으로 최종 예측 값을 예측하는 방식

Boosting:

부스팅은 모델이 학습하기 어려운 데이터 샘플을 찾고 그것을 학습할 수 있는 모델을 또 찾아 이를 연속적으로 연결하는 방식으로 작동한다. 이때 이렇게 모델이 학습하지 못한 데이터 셋에 대해 학습할 수 있는 새로운 모델들을 weak learner라고 한다. 하나의 Strong learner를 만들기 위해 weak learner들의 최적의 연속적인 조합을 하는 방법이라 생각하면 된다.

![baggingandbootstap]({{site.url}}/images/2023-08-31-naver13/baggingandbootstap.png){: .align-center}

위 그림과 같이, Bagging 같은 경우 하나의 데이터 셋에서 sampling을 하고 각각의 sampling 한 데이터 셋에 대해 모델링을 하는 것인 반면, Boosting은 모델을 만들고 그것의 단점을 보완해줄 수 있는 다른 모델들을 또 학습하고 새로운 조합을 만드는 과정 임을 볼 수 있다.
